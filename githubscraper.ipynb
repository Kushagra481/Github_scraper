{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk-y1DMFxv7J",
        "outputId": "0d6e3955-692f-4195-bc8c-2e77b191d6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD-WX9jS1yC5",
        "outputId": "65fe1072-7ca8-4c67-ff59-5a96e6527add"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class HackathonProjectAnalyzer:\n",
        "    def __init__(self, token=None):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with an optional GitHub token.\n",
        "\n",
        "        Args:\n",
        "            token (str, optional): GitHub API token for higher rate limits\n",
        "        \"\"\"\n",
        "        self.base_url = \"https://api.github.com\"\n",
        "        self.headers = {}\n",
        "        if token:\n",
        "            self.headers[\"Authorization\"] = f\"token {token}\"\n",
        "\n",
        "        # Download NLTK resources explicitly\n",
        "        print(\"Downloading required NLTK resources...\")\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "        # Import these after downloading\n",
        "        from nltk.corpus import stopwords\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        self.word_tokenize = word_tokenize\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def search_hackathon_projects(self, limit=500, min_stars=10):\n",
        "        \"\"\"\n",
        "        Search for hackathon projects on GitHub.\n",
        "\n",
        "        Args:\n",
        "            limit (int): Maximum number of repositories to analyze\n",
        "            min_stars (int): Minimum stars to consider\n",
        "\n",
        "        Returns:\n",
        "            list: List of project data dictionaries\n",
        "        \"\"\"\n",
        "        query_terms = [\n",
        "            \"hackathon project\",\n",
        "            \"hackathon submission\",\n",
        "            \"hackathon winner\",\n",
        "            \"24 hour hackathon\",\n",
        "            \"hackathon demo\",\n",
        "            \"hackday project\"\n",
        "        ]\n",
        "\n",
        "        all_projects = []\n",
        "\n",
        "        for query in query_terms:\n",
        "            params = {\n",
        "                \"q\": f\"{query} stars:>={min_stars}\",\n",
        "                \"sort\": \"stars\",\n",
        "                \"order\": \"desc\",\n",
        "                \"per_page\": 100\n",
        "            }\n",
        "\n",
        "            page = 1\n",
        "            while len(all_projects) < limit:\n",
        "                params[\"page\"] = page\n",
        "\n",
        "                try:\n",
        "                    response = requests.get(\n",
        "                        f\"{self.base_url}/search/repositories\",\n",
        "                        headers=self.headers,\n",
        "                        params=params\n",
        "                    )\n",
        "\n",
        "                    if response.status_code == 200:\n",
        "                        data = response.json()\n",
        "                        items = data.get(\"items\", [])\n",
        "\n",
        "                        if not items:\n",
        "                            break\n",
        "\n",
        "                        for repo in items:\n",
        "                            # Extract project info\n",
        "                            project = {\n",
        "                                \"name\": repo[\"name\"],\n",
        "                                \"full_name\": repo[\"full_name\"],\n",
        "                                \"description\": repo[\"description\"] or \"\",\n",
        "                                \"url\": repo[\"html_url\"],\n",
        "                                \"stars\": repo[\"stargazers_count\"],\n",
        "                                \"language\": repo[\"language\"],\n",
        "                                \"topics\": repo.get(\"topics\", []),\n",
        "                                \"created_at\": repo[\"created_at\"]\n",
        "                            }\n",
        "\n",
        "                            # Avoid duplicates\n",
        "                            if project[\"full_name\"] not in [p[\"full_name\"] for p in all_projects]:\n",
        "                                all_projects.append(project)\n",
        "\n",
        "                                if len(all_projects) >= limit:\n",
        "                                    break\n",
        "\n",
        "                        page += 1\n",
        "\n",
        "                        # Handle rate limiting\n",
        "                        remaining = int(response.headers.get(\"X-RateLimit-Remaining\", 0))\n",
        "                        if remaining < 5:\n",
        "                            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", 0))\n",
        "                            sleep_time = max(0, reset_time - time.time()) + 1\n",
        "                            print(f\"Rate limit approaching. Sleeping for {sleep_time:.2f} seconds\")\n",
        "                            time.sleep(sleep_time)\n",
        "\n",
        "                    elif response.status_code == 403:\n",
        "                        # Rate limited\n",
        "                        reset_time = int(response.headers.get(\"X-RateLimit-Reset\", 0))\n",
        "                        sleep_time = max(0, reset_time - time.time()) + 1\n",
        "                        print(f\"Rate limited. Sleeping for {sleep_time:.2f} seconds\")\n",
        "                        time.sleep(sleep_time)\n",
        "\n",
        "                    else:\n",
        "                        print(f\"Error: {response.status_code} - {response.text}\")\n",
        "                        break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Exception occurred: {e}\")\n",
        "                    break\n",
        "\n",
        "        return all_projects\n",
        "\n",
        "    def extract_project_keywords(self, project):\n",
        "        \"\"\"\n",
        "        Extract meaningful keywords from project name and description.\n",
        "\n",
        "        Args:\n",
        "            project (dict): Project information dictionary\n",
        "\n",
        "        Returns:\n",
        "            list: List of keywords\n",
        "        \"\"\"\n",
        "        # Combine name and description\n",
        "        text = f\"{project['name']} {project['description']}\"\n",
        "\n",
        "        # Tokenize and clean\n",
        "        tokens = self.word_tokenize(text.lower())\n",
        "\n",
        "        # Remove stop words and short/irrelevant terms\n",
        "        keywords = [\n",
        "            word for word in tokens\n",
        "            if word.isalpha() and\n",
        "            word not in self.stop_words and\n",
        "            len(word) > 2 and\n",
        "            word not in [\"hackathon\", \"project\", \"app\", \"application\"]\n",
        "        ]\n",
        "\n",
        "        # Add explicit topics and language\n",
        "        if project[\"language\"] and project[\"language\"].lower() not in keywords:\n",
        "            keywords.append(project[\"language\"].lower())\n",
        "\n",
        "        keywords.extend([topic.lower() for topic in project[\"topics\"]])\n",
        "\n",
        "        return keywords\n",
        "\n",
        "    def analyze_without_nltk(self, projects):\n",
        "        \"\"\"\n",
        "        Analyze projects without relying on NLTK (fallback method).\n",
        "\n",
        "        Args:\n",
        "            projects (list): List of project dictionaries\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis results\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        # Analyze languages\n",
        "        languages = [p[\"language\"] for p in projects if p[\"language\"]]\n",
        "        results[\"top_languages\"] = Counter(languages).most_common(10)\n",
        "\n",
        "        # Analyze topics/tags\n",
        "        all_topics = []\n",
        "        for project in projects:\n",
        "            all_topics.extend(project[\"topics\"])\n",
        "        results[\"top_topics\"] = Counter(all_topics).most_common(15)\n",
        "\n",
        "        # Simple keyword extraction without NLTK\n",
        "        common_words = set([\n",
        "            \"and\", \"the\", \"a\", \"an\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\",\n",
        "            \"by\", \"from\", \"about\", \"as\", \"that\", \"this\", \"is\", \"are\", \"was\", \"were\",\n",
        "            \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\",\n",
        "            \"would\", \"should\", \"could\", \"can\", \"may\", \"might\", \"must\", \"project\", \"app\",\n",
        "            \"hackathon\", \"code\", \"application\", \"demo\", \"create\", \"using\", \"use\", \"used\"\n",
        "        ])\n",
        "\n",
        "        all_keywords = []\n",
        "        for project in projects:\n",
        "            # Simple word extraction without NLTK\n",
        "            words = re.findall(r'\\b[a-zA-Z]{3,}\\b', f\"{project['name']} {project['description']}\")\n",
        "            keywords = [word.lower() for word in words if word.lower() not in common_words]\n",
        "            all_keywords.extend(keywords)\n",
        "            all_keywords.extend([topic.lower() for topic in project[\"topics\"]])\n",
        "            if project[\"language\"]:\n",
        "                all_keywords.append(project[\"language\"].lower())\n",
        "\n",
        "        results[\"top_keywords\"] = Counter(all_keywords).most_common(20)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def export_simplified_results(self, projects, trends, filename_prefix=\"hackathon_projects\"):\n",
        "        \"\"\"\n",
        "        Export simplified analysis results to CSV files.\n",
        "\n",
        "        Args:\n",
        "            projects (list): List of project dictionaries\n",
        "            trends (dict): Trend analysis results\n",
        "            filename_prefix (str): Prefix for output filenames\n",
        "        \"\"\"\n",
        "        # Export projects\n",
        "        df_projects = pd.DataFrame(projects)\n",
        "        if \"topics\" in df_projects.columns:\n",
        "            df_projects[\"topics\"] = df_projects[\"topics\"].apply(lambda x: \", \".join(x) if x else \"\")\n",
        "        df_projects.to_csv(f\"{filename_prefix}_data.csv\", index=False)\n",
        "\n",
        "        # Group projects by language\n",
        "        language_groups = {}\n",
        "        for project in projects:\n",
        "            lang = project.get(\"language\") or \"Unknown\"\n",
        "            if lang not in language_groups:\n",
        "                language_groups[lang] = []\n",
        "            language_groups[lang].append(project)\n",
        "\n",
        "        language_data = []\n",
        "        for lang, projs in language_groups.items():\n",
        "            language_data.append({\n",
        "                \"language\": lang,\n",
        "                \"count\": len(projs),\n",
        "                \"average_stars\": sum(p[\"stars\"] for p in projs) / len(projs),\n",
        "                \"examples\": \", \".join([p[\"full_name\"] for p in sorted(projs, key=lambda x: x[\"stars\"], reverse=True)[:3]])\n",
        "            })\n",
        "\n",
        "        df_languages = pd.DataFrame(language_data)\n",
        "        df_languages.to_csv(f\"{filename_prefix}_by_language.csv\", index=False)\n",
        "\n",
        "        # Group projects by topic\n",
        "        topic_data = []\n",
        "        for topic, count in trends[\"top_topics\"]:\n",
        "            topic_projects = [p for p in projects if topic in p[\"topics\"]]\n",
        "            topic_data.append({\n",
        "                \"topic\": topic,\n",
        "                \"count\": count,\n",
        "                \"examples\": \", \".join([p[\"full_name\"] for p in sorted(topic_projects, key=lambda x: x[\"stars\"], reverse=True)[:3]])\n",
        "            })\n",
        "\n",
        "        df_topics = pd.DataFrame(topic_data)\n",
        "        df_topics.to_csv(f\"{filename_prefix}_by_topic.csv\", index=False)\n",
        "\n",
        "        # Export keyword trends\n",
        "        keyword_data = []\n",
        "        for keyword, count in trends[\"top_keywords\"]:\n",
        "            keyword_data.append({\n",
        "                \"keyword\": keyword,\n",
        "                \"count\": count\n",
        "            })\n",
        "\n",
        "        df_keywords = pd.DataFrame(keyword_data)\n",
        "        df_keywords.to_csv(f\"{filename_prefix}_keywords.csv\", index=False)\n",
        "\n",
        "        print(f\"Data exported to {filename_prefix}_*.csv files\")\n",
        "\n",
        "def main():\n",
        "    # You can get a GitHub token by following instructions at:\n",
        "    # https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\n",
        "    token = os.environ.get(\"GITHUB_TOKEN\")  # Or replace with your token\n",
        "\n",
        "    analyzer = HackathonProjectAnalyzer(token=token)\n",
        "\n",
        "    print(\"Searching for hackathon projects...\")\n",
        "    projects = analyzer.search_hackathon_projects(limit=300)\n",
        "\n",
        "    if projects:\n",
        "        print(f\"Found {len(projects)} hackathon projects\")\n",
        "\n",
        "        try:\n",
        "            # Try to use NLTK features\n",
        "            print(\"Analyzing project keywords...\")\n",
        "            all_keywords = []\n",
        "            for project in projects:\n",
        "                keywords = analyzer.extract_project_keywords(project)\n",
        "                all_keywords.extend(keywords)\n",
        "\n",
        "            keyword_counts = Counter(all_keywords).most_common(20)\n",
        "            print(\"\\nTop project keywords:\")\n",
        "            for keyword, count in keyword_counts:\n",
        "                print(f\"{keyword}: {count} occurrences\")\n",
        "\n",
        "            # Try to cluster if sklearn is available\n",
        "            try:\n",
        "                from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "                from sklearn.cluster import KMeans\n",
        "\n",
        "                # Create documents from project descriptions\n",
        "                documents = []\n",
        "                for project in projects:\n",
        "                    doc = f\"{project['name']} {project['description']} {' '.join(project['topics'])}\"\n",
        "                    documents.append(doc)\n",
        "\n",
        "                # Use TF-IDF to vectorize documents\n",
        "                vectorizer = TfidfVectorizer(\n",
        "                    max_features=1000,\n",
        "                    stop_words='english',\n",
        "                    ngram_range=(1, 2)\n",
        "                )\n",
        "\n",
        "                X = vectorizer.fit_transform(documents)\n",
        "\n",
        "                # Perform KMeans clustering\n",
        "                kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "                kmeans.fit(X)\n",
        "\n",
        "                # Assign clusters to projects\n",
        "                for i, project in enumerate(projects):\n",
        "                    project[\"cluster\"] = int(kmeans.labels_[i])\n",
        "\n",
        "                # Group projects by cluster\n",
        "                clusters = {}\n",
        "                for project in projects:\n",
        "                    cluster_id = project[\"cluster\"]\n",
        "                    if cluster_id not in clusters:\n",
        "                        clusters[cluster_id] = []\n",
        "                    clusters[cluster_id].append(project)\n",
        "\n",
        "                print(\"\\nProject clusters:\")\n",
        "                for cluster_id, cluster_projects in clusters.items():\n",
        "                    print(f\"Cluster {cluster_id} ({len(cluster_projects)} projects):\")\n",
        "                    for p in sorted(cluster_projects, key=lambda x: x[\"stars\"], reverse=True)[:3]:\n",
        "                        print(f\"  - {p['full_name']}: {p['description'][:100]}...\")\n",
        "\n",
        "            except ImportError:\n",
        "                print(\"sklearn not available, skipping clustering\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error using NLTK: {e}\")\n",
        "            print(\"Falling back to simplified analysis\")\n",
        "\n",
        "        # Always perform basic analysis\n",
        "        print(\"\\nAnalyzing project trends...\")\n",
        "        trends = analyzer.analyze_without_nltk(projects)\n",
        "\n",
        "        print(\"\\nTop programming languages:\")\n",
        "        for language, count in trends[\"top_languages\"]:\n",
        "            print(f\"{language}: {count} projects\")\n",
        "\n",
        "        print(\"\\nMost common hackathon topics/tags:\")\n",
        "        for topic, count in trends[\"top_topics\"]:\n",
        "            print(f\"{topic}: {count} occurrences\")\n",
        "\n",
        "        # Export data\n",
        "        analyzer.export_simplified_results(projects, trends)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V97jo8Vlx7gK",
        "outputId": "f066456c-40a4-4613-ffdf-92f9b1b45b88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading required NLTK resources...\n",
            "Searching for hackathon projects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit approaching. Sleeping for 51.53 seconds\n",
            "Found 300 hackathon projects\n",
            "Analyzing project keywords...\n",
            "Error using NLTK: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Falling back to simplified analysis\n",
            "\n",
            "Analyzing project trends...\n",
            "\n",
            "Top programming languages:\n",
            "JavaScript: 57 projects\n",
            "Python: 42 projects\n",
            "TypeScript: 34 projects\n",
            "HTML: 23 projects\n",
            "Jupyter Notebook: 22 projects\n",
            "CSS: 11 projects\n",
            "Java: 11 projects\n",
            "Dart: 10 projects\n",
            "Solidity: 9 projects\n",
            "Objective-C: 6 projects\n",
            "\n",
            "Most common hackathon topics/tags:\n",
            "hackathon: 60 occurrences\n",
            "python: 21 occurrences\n",
            "machine-learning: 17 occurrences\n",
            "hacktoberfest: 14 occurrences\n",
            "hackathon-project: 14 occurrences\n",
            "javascript: 13 occurrences\n",
            "react: 10 occurrences\n",
            "deep-learning: 8 occurrences\n",
            "open-source: 8 occurrences\n",
            "blockchain: 8 occurrences\n",
            "flutter: 8 occurrences\n",
            "typescript: 7 occurrences\n",
            "nextjs: 7 occurrences\n",
            "nodejs: 6 occurrences\n",
            "artificial-intelligence: 6 occurrences\n",
            "Data exported to hackathon_projects_*.csv files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this to the end of your main() function\n",
        "print(\"\\nPrinting contents of the CSV files:\")\n",
        "\n",
        "# Print projects data\n",
        "print(\"\\n=== PROJECTS DATA ===\")\n",
        "projects_df = pd.read_csv(\"hackathon_projects_data.csv\")\n",
        "print(projects_df.head())  # Print first 5 rows\n",
        "\n",
        "# Print language data\n",
        "print(\"\\n=== LANGUAGE DATA ===\")\n",
        "lang_df = pd.read_csv(\"hackathon_projects_by_language.csv\")\n",
        "print(lang_df)  # Print all rows (usually not too many)\n",
        "\n",
        "# Print topic data\n",
        "print(\"\\n=== TOPIC DATA ===\")\n",
        "topic_df = pd.read_csv(\"hackathon_projects_by_topic.csv\")\n",
        "print(topic_df)  # Print all rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4IzcSlDyfrI",
        "outputId": "af57f4f4-c581-4771-a20f-6cfa99bf1d33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Printing contents of the CSV files:\n",
            "\n",
            "=== PROJECTS DATA ===\n",
            "                           name                              full_name  \\\n",
            "0           graphql-starter-kit           kriasoft/graphql-starter-kit   \n",
            "1    awesome-hackathon-projects  Olanetsoft/awesome-hackathon-projects   \n",
            "2                          Otto                       KartikChugh/Otto   \n",
            "3   mlh-hackathon-flask-starter        MLH/mlh-hackathon-flask-starter   \n",
            "4  mlh-hackathon-nodejs-starter       MLH/mlh-hackathon-nodejs-starter   \n",
            "\n",
            "                                         description  \\\n",
            "0  ðŸ’¥  Monorepo template (seed project) pre-config...   \n",
            "1  This is a curated list of amazing hackathon pr...   \n",
            "2  Otto makes machine learning an intuitive, natu...   \n",
            "3   Hackathon starter project for Flask applications   \n",
            "4  Hackathon starter project for Node.js applicat...   \n",
            "\n",
            "                                                 url  stars    language  \\\n",
            "0    https://github.com/kriasoft/graphql-starter-kit   3919  TypeScript   \n",
            "1  https://github.com/Olanetsoft/awesome-hackatho...   1295         NaN   \n",
            "2                https://github.com/KartikChugh/Otto    950  JavaScript   \n",
            "3  https://github.com/MLH/mlh-hackathon-flask-sta...    735        HTML   \n",
            "4  https://github.com/MLH/mlh-hackathon-nodejs-st...    698        HTML   \n",
            "\n",
            "                                              topics            created_at  \n",
            "0  api, boilerplate, cloud-run, graphql, graphql-...  2016-09-30T13:14:17Z  \n",
            "1  awesome, awesome-list, hackathon, hacktoberfes...  2021-01-02T12:57:36Z  \n",
            "2  artificial-intelligence, automl, chat-applicat...  2020-05-25T19:42:57Z  \n",
            "3               flask, hackathon-starter, hackathons  2019-01-11T16:07:22Z  \n",
            "4              hackathon-starter, hackathons, nodejs  2019-02-25T15:37:02Z  \n",
            "\n",
            "=== LANGUAGE DATA ===\n",
            "            language  count  average_stars  \\\n",
            "0         TypeScript     34     171.941176   \n",
            "1            Unknown     38      64.842105   \n",
            "2         JavaScript     57      45.736842   \n",
            "3               HTML     23      89.000000   \n",
            "4                CSS     11      59.000000   \n",
            "5        Objective-C      6      53.833333   \n",
            "6               Java     11      31.909091   \n",
            "7             Python     42      27.714286   \n",
            "8           Solidity      9      29.777778   \n",
            "9                  C      2      50.000000   \n",
            "10  Jupyter Notebook     22      27.045455   \n",
            "11               PHP      4      24.750000   \n",
            "12          GDScript      1      48.000000   \n",
            "13        Dockerfile      1      39.000000   \n",
            "14              Dart     10      17.500000   \n",
            "15                 R      1      30.000000   \n",
            "16                C#      5      24.800000   \n",
            "17               Vue      3      17.666667   \n",
            "18               TeX      1      20.000000   \n",
            "19                Go      5      17.000000   \n",
            "20             Swift      3      13.666667   \n",
            "21              Rust      3      14.333333   \n",
            "22               C++      1      16.000000   \n",
            "23               EJS      1      14.000000   \n",
            "24             Scala      1      13.000000   \n",
            "25         ShaderLab      1      11.000000   \n",
            "26             Shell      2      46.500000   \n",
            "27           Arduino      1      10.000000   \n",
            "28            Ren'Py      1      10.000000   \n",
            "\n",
            "                                             examples  \n",
            "0   kriasoft/graphql-starter-kit, ahmetuysal/nest-...  \n",
            "1   Olanetsoft/awesome-hackathon-projects, appwrit...  \n",
            "2   KartikChugh/Otto, Ronak-59/Stock-Prediction, C...  \n",
            "3   MLH/mlh-hackathon-flask-starter, MLH/mlh-hacka...  \n",
            "4   diafygi/gnu-pricing, kaiwalyakoparkar/sense-ha...  \n",
            "5   hACKbUSTER/UberGuide-iOS, eduardomoroni/react-...  \n",
            "6   soumyadip007/E-Medical-System-Web-Project-Usin...  \n",
            "7   jxtxzzw/Microsoft-Hackathon-2019-Intelligent-G...  \n",
            "8   coredao-org/Build-On-Core, nrxschool/hackathon...  \n",
            "9                  grafana/doom-datasource, hkjn/lnhw  \n",
            "10  yh08037/quantum-neural-network, RJ-Heisenberg/...  \n",
            "11  imlakshay08/waste-management-system, akshatvg/...  \n",
            "12                       KOBUGE-Games/project-kumquat  \n",
            "13               vukan-markovic/Github-Android-Action  \n",
            "14  uzibytes/Voco_App, tsinis/plan_et_b, 8Bit1Byte...  \n",
            "15                       biocswirl-dev-team/BiocSwirl  \n",
            "16  anthonypuppo/sk-nl2ef-plugin, nicoazel/ArcRhin...  \n",
            "17  esclapes/coosto-hackathon, awakentrue/simple-h...  \n",
            "18                          MarioniLab/EmptyDrops2017  \n",
            "19  keeferrourke/imgrep, DMW2151/expert-garbanzo, ...  \n",
            "20  kvld/junction2018, anthonyanader/timeboARd, mi...  \n",
            "21  da-bao-jian/luban-the-paymaster, f321x/cashu-e...  \n",
            "22                               leapmotion/ar-screen  \n",
            "23                                     mondher0/Datum  \n",
            "24                         NazeriMahdi2001/Raffle-Doc  \n",
            "25                     DanMillerDev/GameJamStarterKit  \n",
            "26  ScottBrenner/generate-changelog-action, nextfl...  \n",
            "27                                       noopkat/purr  \n",
            "28                                sonymanetov/Avocats  \n",
            "\n",
            "=== TOPIC DATA ===\n",
            "                      topic  count  \\\n",
            "0                 hackathon     60   \n",
            "1                    python     21   \n",
            "2          machine-learning     17   \n",
            "3             hacktoberfest     14   \n",
            "4         hackathon-project     14   \n",
            "5                javascript     13   \n",
            "6                     react     10   \n",
            "7             deep-learning      8   \n",
            "8               open-source      8   \n",
            "9                blockchain      8   \n",
            "10                  flutter      8   \n",
            "11               typescript      7   \n",
            "12                   nextjs      7   \n",
            "13                   nodejs      6   \n",
            "14  artificial-intelligence      6   \n",
            "\n",
            "                                             examples  \n",
            "0   kriasoft/graphql-starter-kit, Olanetsoft/aweso...  \n",
            "1   Olanetsoft/awesome-hackathon-projects, KartikC...  \n",
            "2   KartikChugh/Otto, Ronak-59/Stock-Prediction, o...  \n",
            "3   Olanetsoft/awesome-hackathon-projects, appwrit...  \n",
            "4   Susmita-Dey/Sukoon, imlakshay08/waste-manageme...  \n",
            "5   Olanetsoft/awesome-hackathon-projects, tudorco...  \n",
            "6   kriasoft/graphql-starter-kit, Olanetsoft/aweso...  \n",
            "7   KartikChugh/Otto, SatelliteVu/SatelliteVu-AWS-...  \n",
            "8   Chimoney/chimoney-community-projects, Susmita-...  \n",
            "9   lexvanderstoep/MedicalBlockchain, Kashyapdeves...  \n",
            "10  uzibytes/Voco_App, Ayushpanditmoto/projectshub...  \n",
            "11  kriasoft/graphql-starter-kit, ahmetuysal/nest-...  \n",
            "12  pheralb/project-hackathon, Ayushpanditmoto/pro...  \n",
            "13  kriasoft/graphql-starter-kit, MLH/mlh-hackatho...  \n",
            "14  KartikChugh/Otto, Ronak-59/Stock-Prediction, a...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_m12sel2rCs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}